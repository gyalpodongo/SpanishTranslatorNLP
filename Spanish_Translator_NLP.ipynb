{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spanish_Translator_NLP.ipynb","provenance":[{"file_id":"1i4a26jVRVsAIfAGCe5pnl4l51B6GvjWs","timestamp":1622798101134},{"file_id":"17dOcF-VlAVBY0vzgqaANGQ4taSNc2Wp6","timestamp":1571411676663}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m9JJ7FBw84tG"},"source":["# Stage 1: Importing dependencies"]},{"cell_type":"code","metadata":{"id":"ZbcvtPlp3YWu","executionInfo":{"status":"ok","timestamp":1622800526740,"user_tz":-60,"elapsed":666,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["import numpy as np\n","import math\n","import re\n","import time\n","from google.colab import drive"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6o_cpZz3y_-","executionInfo":{"status":"ok","timestamp":1622800527536,"user_tz":-60,"elapsed":17,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["try:\n","    %tensorflow_version 2.x\n","except:\n","    pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","import tensorflow_datasets as tfds"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQN8jwx48_yU"},"source":["# Stage 2: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"bPlOT-2mlw0r"},"source":["## Loading files"]},{"cell_type":"markdown","metadata":{"id":"dCD9jwXsLwS_"},"source":["We import files from our personal google drive."]},{"cell_type":"code","metadata":{"id":"eQpbl1pXCR0p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622800527538,"user_tz":-60,"elapsed":17,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}},"outputId":"295853d5-2273-40f9-aea4-c091c10d2928"},"source":["drive.mount(\"/content/drive\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q8Or0sLV5b8t","executionInfo":{"status":"ok","timestamp":1622800533691,"user_tz":-60,"elapsed":6163,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["with open(\"/content/drive/MyDrive/NLP Course/Transformer/data/europarl-v7.es-en.en\",\n","          mode='r',\n","          encoding='utf-8') as f:\n","    europarl_en = f.read()\n","with open(\"/content/drive/MyDrive/NLP Course/Transformer/data/europarl-v7.es-en.es\",\n","          mode='r',\n","          encoding='utf-8') as f:\n","    europarl_es = f.read()\n","with open(\"/content/drive/MyDrive/NLP Course/Transformer/data/nonbreaking_prefix.en\",\n","          mode='r',\n","          encoding='utf-8') as f:\n","    non_breaking_prefix_en = f.read()\n","with open(\"/content/drive/MyDrive/NLP Course/Transformer/data/nonbreaking_prefix.es\",\n","          mode='r',\n","          encoding='utf-8') as f:\n","    non_breaking_prefix_es = f.read()"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TEFw0D2vP_Dl"},"source":["## Cleaning data"]},{"cell_type":"markdown","metadata":{"id":"PwIBeGXn7LIJ"},"source":["Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."]},{"cell_type":"code","metadata":{"id":"L_TeuktU40Cb","executionInfo":{"status":"ok","timestamp":1622800533699,"user_tz":-60,"elapsed":14,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n","non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n","non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n","non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9x4mZfKMaxD"},"source":["We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."]},{"cell_type":"code","metadata":{"id":"Qg-8LLK-WdFp","executionInfo":{"status":"ok","timestamp":1622800633716,"user_tz":-60,"elapsed":100028,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["corpus_en = europarl_en\n","# Add $$$ after non ending sentence points\n","for prefix in non_breaking_prefix_en:\n","    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n","corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n","# Remove $$$ markers\n","corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n","# Clear multiple spaces\n","corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n","corpus_en = corpus_en.split('\\n')\n","\n","corpus_es = europarl_es\n","for prefix in non_breaking_prefix_es:\n","    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n","corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n","corpus_es = re.sub(r\".\\$\\$\\$\", '', corpus_es)\n","corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n","corpus_es = corpus_es.split('\\n')"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-Y9v8-Tozl2"},"source":["## Tokenizing text"]},{"cell_type":"code","metadata":{"id":"p5YXanmOd_xK","executionInfo":{"status":"ok","timestamp":1622801531275,"user_tz":-60,"elapsed":897600,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    corpus_en, target_vocab_size=2**13)\n","tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    corpus_es, target_vocab_size=2**13)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftIbPzIwCtwL","executionInfo":{"status":"ok","timestamp":1622801531280,"user_tz":-60,"elapsed":75,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n","VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2 # = 8171"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPFe2YJDC9jw","executionInfo":{"status":"ok","timestamp":1622801808332,"user_tz":-60,"elapsed":277068,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n","          for sentence in corpus_en]\n","outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n","           for sentence in corpus_es]"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bG6AlcFMpC5C"},"source":["## Remove too long sentences"]},{"cell_type":"code","metadata":{"id":"F6CD6PLGyQWy","executionInfo":{"status":"ok","timestamp":1622802098925,"user_tz":-60,"elapsed":290641,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["MAX_LENGTH = 20\n","idx_to_remove = [count for count, sent in enumerate(inputs)\n","                 if len(sent) > MAX_LENGTH]\n","for idx in reversed(idx_to_remove):\n","    del inputs[idx]\n","    del outputs[idx]\n","idx_to_remove = [count for count, sent in enumerate(outputs)\n","                 if len(sent) > MAX_LENGTH]\n","for idx in reversed(idx_to_remove):\n","    del inputs[idx]\n","    del outputs[idx]"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ypm8h5aZQTZ1"},"source":["## Inputs/outputs creation"]},{"cell_type":"markdown","metadata":{"id":"9FP0WPsdM8hl"},"source":["As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."]},{"cell_type":"code","metadata":{"id":"nvDfLDWUONlE","executionInfo":{"status":"ok","timestamp":1622802103699,"user_tz":-60,"elapsed":4794,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n","                                                       value=0,\n","                                                       padding='post',\n","                                                       maxlen=MAX_LENGTH)\n","outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n","                                                        value=0,\n","                                                        padding='post',\n","                                                        maxlen=MAX_LENGTH)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFxMp3TOIYff","executionInfo":{"status":"ok","timestamp":1622802103702,"user_tz":-60,"elapsed":117,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycT0YqydRcUd"},"source":["# Stage 3: Model building"]},{"cell_type":"markdown","metadata":{"id":"-SBoH8G4XyR9"},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{"id":"7G9C3ucmJ86I"},"source":["Positional encoding formulae:\n","\n","$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n","\n","$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"]},{"cell_type":"code","metadata":{"id":"e2wc6sYlX0dr","executionInfo":{"status":"ok","timestamp":1622802104122,"user_tz":-60,"elapsed":64,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class PositionalEncoding(layers.Layer):\n","\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","    \n","    def get_angles(self, pos, i, d_model):\n","        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n","        return pos * angles\n","\n","    def call(self, inputs):\n","        seq_length = inputs.shape.as_list()[-2]\n","        d_model = inputs.shape.as_list()[-1]\n","        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n","                                 np.arange(d_model)[np.newaxis, :],\n","                                 d_model)\n","        angles[:, 0::2] = np.sin(angles[:, 0::2])\n","        angles[:, 1::2] = np.cos(angles[:, 1::2])\n","        pos_encoding = angles[np.newaxis, ...]\n","        return inputs + tf.cast(pos_encoding, tf.float32)"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcw8YIQqRhOJ"},"source":["## Attention"]},{"cell_type":"markdown","metadata":{"id":"3sffhwwvX-wj"},"source":["### Attention computation"]},{"cell_type":"markdown","metadata":{"id":"7VBuW6lESLDX"},"source":["$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"]},{"cell_type":"code","metadata":{"id":"2rEoCNJURbrT","executionInfo":{"status":"ok","timestamp":1622802104123,"user_tz":-60,"elapsed":63,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["def scaled_dot_product_attention(queries, keys, values, mask):\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","    \n","    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n","    scaled_product = product / tf.math.sqrt(keys_dim)\n","    \n","    if mask is not None:\n","        scaled_product += (mask * -1e9)\n","    \n","    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n","    \n","    return attention"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MjtvXrfYEx7"},"source":["### Multi-head attention sublayer"]},{"cell_type":"code","metadata":{"id":"lvq4I9uTX5p7","executionInfo":{"status":"ok","timestamp":1622802104124,"user_tz":-60,"elapsed":63,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class MultiHeadAttention(layers.Layer):\n","    \n","    def __init__(self, nb_proj):\n","        super(MultiHeadAttention, self).__init__()\n","        self.nb_proj = nb_proj\n","        \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        assert self.d_model % self.nb_proj == 0\n","        \n","        self.d_proj = self.d_model // self.nb_proj\n","        \n","        self.query_lin = layers.Dense(units=self.d_model)\n","        self.key_lin = layers.Dense(units=self.d_model)\n","        self.value_lin = layers.Dense(units=self.d_model)\n","        \n","        self.final_lin = layers.Dense(units=self.d_model)\n","        \n","    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n","        shape = (batch_size,\n","                 -1,\n","                 self.nb_proj,\n","                 self.d_proj)\n","        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n","        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n","    \n","    def call(self, queries, keys, values, mask):\n","        batch_size = tf.shape(queries)[0]\n","        \n","        queries = self.query_lin(queries)\n","        keys = self.key_lin(keys)\n","        values = self.value_lin(values)\n","        \n","        queries = self.split_proj(queries, batch_size)\n","        keys = self.split_proj(keys, batch_size)\n","        values = self.split_proj(values, batch_size)\n","        \n","        attention = scaled_dot_product_attention(queries, keys, values, mask)\n","        \n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        \n","        concat_attention = tf.reshape(attention,\n","                                      shape=(batch_size, -1, self.d_model))\n","        \n","        outputs = self.final_lin(concat_attention)\n","        \n","        return outputs"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiyuHe1OeT5N"},"source":["## Encoder"]},{"cell_type":"code","metadata":{"id":"UV0ZMH7KT_KZ","executionInfo":{"status":"ok","timestamp":1622802104125,"user_tz":-60,"elapsed":62,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class EncoderLayer(layers.Layer):\n","    \n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\n","        super(EncoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout_rate = dropout_rate\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        \n","        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","    def call(self, inputs, mask, training):\n","        attention = self.multi_head_attention(inputs,\n","                                              inputs,\n","                                              inputs,\n","                                              mask)\n","        attention = self.dropout_1(attention, training=training)\n","        attention = self.norm_1(attention + inputs)\n","        \n","        outputs = self.dense_1(attention)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_2(outputs, training=training)\n","        outputs = self.norm_2(outputs + attention)\n","        \n","        return outputs"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-P92KeZih60","executionInfo":{"status":"ok","timestamp":1622802104125,"user_tz":-60,"elapsed":61,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class Encoder(layers.Layer):\n","    \n","    def __init__(self,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout_rate,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"encoder\"):\n","        super(Encoder, self).__init__(name=name)\n","        self.nb_layers = nb_layers\n","        self.d_model = d_model\n","        \n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        self.enc_layers = [EncoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout_rate) \n","                           for _ in range(nb_layers)]\n","    \n","    def call(self, inputs, mask, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        \n","        for i in range(self.nb_layers):\n","            outputs = self.enc_layers[i](outputs, mask, training)\n","\n","        return outputs"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DthraBEwuvl"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"7ZWZyFBnwy8u","executionInfo":{"status":"ok","timestamp":1622802104127,"user_tz":-60,"elapsed":61,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class DecoderLayer(layers.Layer):\n","    \n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\n","        super(DecoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout_rate = dropout_rate\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        \n","        # Self multi head attention\n","        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        # Multi head attention combined with encoder output\n","        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        # Feed foward\n","        self.dense_1 = layers.Dense(units=self.FFN_units,\n","                                    activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        attention = self.multi_head_attention_1(inputs,\n","                                                inputs,\n","                                                inputs,\n","                                                mask_1)\n","        attention = self.dropout_1(attention, training)\n","        attention = self.norm_1(attention + inputs)\n","        \n","        attention_2 = self.multi_head_attention_2(attention,\n","                                                  enc_outputs,\n","                                                  enc_outputs,\n","                                                  mask_2)\n","        attention_2 = self.dropout_2(attention_2, training)\n","        attention_2 = self.norm_2(attention_2 + attention)\n","        \n","        outputs = self.dense_1(attention_2)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_3(outputs, training)\n","        outputs = self.norm_3(outputs + attention_2)\n","        \n","        return outputs"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpzdiWHiwywF","executionInfo":{"status":"ok","timestamp":1622802104128,"user_tz":-60,"elapsed":61,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class Decoder(layers.Layer):\n","    \n","    def __init__(self,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout_rate,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"decoder\"):\n","        super(Decoder, self).__init__(name=name)\n","        self.d_model = d_model\n","        self.nb_layers = nb_layers\n","        \n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        \n","        self.dec_layers = [DecoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout_rate) \n","                           for i in range(nb_layers)]\n","    \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        \n","        for i in range(self.nb_layers):\n","            outputs = self.dec_layers[i](outputs,\n","                                         enc_outputs,\n","                                         mask_1,\n","                                         mask_2,\n","                                         training)\n","\n","        return outputs"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5sJYkjbz5DD"},"source":["## Transformer"]},{"cell_type":"code","metadata":{"id":"GqvqNjJPwyh-","executionInfo":{"status":"ok","timestamp":1622802104128,"user_tz":-60,"elapsed":60,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class Transformer(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 vocab_size_enc,\n","                 vocab_size_dec,\n","                 d_model,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout_rate,\n","                 name=\"transformer\"):\n","        super(Transformer, self).__init__(name=name)\n","        \n","        self.encoder = Encoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout_rate,\n","                               vocab_size_enc,\n","                               d_model)\n","        self.decoder = Decoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout_rate,\n","                               vocab_size_dec,\n","                               d_model)\n","        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n","    \n","    def create_padding_mask(self, seq):\n","        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","        return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","    def create_look_ahead_mask(self, seq):\n","        seq_len = tf.shape(seq)[1]\n","        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","        return look_ahead_mask\n","    \n","    def call(self, enc_inputs, dec_inputs, training):\n","        enc_mask = self.create_padding_mask(enc_inputs)\n","        dec_mask_1 = tf.maximum(\n","            self.create_padding_mask(dec_inputs),\n","            self.create_look_ahead_mask(dec_inputs)\n","        )\n","        dec_mask_2 = self.create_padding_mask(enc_inputs)\n","        \n","        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n","        dec_outputs = self.decoder(dec_inputs,\n","                                   enc_outputs,\n","                                   dec_mask_1,\n","                                   dec_mask_2,\n","                                   training)\n","        \n","        outputs = self.last_linear(dec_outputs)\n","        \n","        return outputs"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-c-LRThUPrso"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"qiOdqQ5qPs8z","executionInfo":{"status":"ok","timestamp":1622802104129,"user_tz":-60,"elapsed":60,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["tf.keras.backend.clear_session()\n","\n","# Hyper-parameters\n","D_MODEL = 128 # 512\n","NB_LAYERS = 4 # 6\n","FFN_UNITS = 512 # 2048\n","NB_PROJ = 8 # 8\n","DROPOUT_RATE = 0.1 # 0.1\n","\n","transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n","                          vocab_size_dec=VOCAB_SIZE_ES,\n","                          d_model=D_MODEL,\n","                          nb_layers=NB_LAYERS,\n","                          FFN_units=FFN_UNITS,\n","                          nb_proj=NB_PROJ,\n","                          dropout_rate=DROPOUT_RATE)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"46xg4Wrg1Wgl","executionInfo":{"status":"ok","timestamp":1622802104130,"user_tz":-60,"elapsed":60,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                            reduction=\"none\")\n","\n","def loss_function(target, pred):\n","    mask = tf.math.logical_not(tf.math.equal(target, 0))\n","    loss_ = loss_object(target, pred)\n","    \n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Goque362343","executionInfo":{"status":"ok","timestamp":1622802104132,"user_tz":-60,"elapsed":60,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        \n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        \n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","leaning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(leaning_rate,\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-9)\n","        "],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nb_32PIU5Zkh","executionInfo":{"status":"ok","timestamp":1622802104133,"user_tz":-60,"elapsed":61,"user":{"displayName":"Gyalpo Dongo Aguirre","photoUrl":"","userId":"04179495148534616806"}}},"source":["checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print(\"Latest checkpoint restored!!\")"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhFK5kUx602K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a0d57b3-a5f4-432f-d4cc-37859804c7b8"},"source":["EPOCHS = 5\n","for epoch in range(EPOCHS):\n","    print(\"Start of epoch {}\".format(epoch+1))\n","    start = time.time()\n","    \n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    \n","    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n","        dec_inputs = targets[:, :-1]\n","        dec_outputs_real = targets[:, 1:]\n","        with tf.GradientTape() as tape:\n","            predictions = transformer(enc_inputs, dec_inputs, True)\n","            loss = loss_function(dec_outputs_real, predictions)\n","        \n","        gradients = tape.gradient(loss, transformer.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","        \n","        train_loss(loss)\n","        train_accuracy(dec_outputs_real, predictions)\n","        \n","        if batch % 50 == 0:\n","            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n","                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n","            \n","    ckpt_save_path = ckpt_manager.save()\n","    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n","                                                        ckpt_save_path))\n","    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start of epoch 1\n","Epoch 1 Batch 0 Loss 6.2360 Accuracy 0.0000\n","Epoch 1 Batch 50 Loss 6.0755 Accuracy 0.0082\n","Epoch 1 Batch 100 Loss 6.0048 Accuracy 0.0298\n","Epoch 1 Batch 150 Loss 5.9621 Accuracy 0.0374\n","Epoch 1 Batch 200 Loss 5.8661 Accuracy 0.0412\n","Epoch 1 Batch 250 Loss 5.7717 Accuracy 0.0435\n","Epoch 1 Batch 300 Loss 5.6595 Accuracy 0.0451\n","Epoch 1 Batch 350 Loss 5.5451 Accuracy 0.0503\n","Epoch 1 Batch 400 Loss 5.4337 Accuracy 0.0556\n","Epoch 1 Batch 450 Loss 5.3234 Accuracy 0.0600\n","Epoch 1 Batch 500 Loss 5.2210 Accuracy 0.0642\n","Epoch 1 Batch 550 Loss 5.1289 Accuracy 0.0686\n","Epoch 1 Batch 600 Loss 5.0411 Accuracy 0.0730\n","Epoch 1 Batch 650 Loss 4.9571 Accuracy 0.0772\n","Epoch 1 Batch 700 Loss 4.8786 Accuracy 0.0815\n","Epoch 1 Batch 750 Loss 4.8086 Accuracy 0.0859\n","Epoch 1 Batch 800 Loss 4.7381 Accuracy 0.0903\n","Epoch 1 Batch 850 Loss 4.6743 Accuracy 0.0946\n","Epoch 1 Batch 900 Loss 4.6120 Accuracy 0.0989\n","Epoch 1 Batch 950 Loss 4.5536 Accuracy 0.1029\n","Epoch 1 Batch 1000 Loss 4.4956 Accuracy 0.1069\n","Epoch 1 Batch 1050 Loss 4.4414 Accuracy 0.1105\n","Epoch 1 Batch 1100 Loss 4.3887 Accuracy 0.1139\n","Epoch 1 Batch 1150 Loss 4.3383 Accuracy 0.1171\n","Epoch 1 Batch 1200 Loss 4.2917 Accuracy 0.1202\n","Epoch 1 Batch 1250 Loss 4.2462 Accuracy 0.1232\n","Epoch 1 Batch 1300 Loss 4.2033 Accuracy 0.1262\n","Epoch 1 Batch 1350 Loss 4.1621 Accuracy 0.1290\n","Epoch 1 Batch 1400 Loss 4.1204 Accuracy 0.1317\n","Epoch 1 Batch 1450 Loss 4.0831 Accuracy 0.1343\n","Epoch 1 Batch 1500 Loss 4.0465 Accuracy 0.1368\n","Epoch 1 Batch 1550 Loss 4.0113 Accuracy 0.1393\n","Epoch 1 Batch 1600 Loss 3.9763 Accuracy 0.1417\n","Epoch 1 Batch 1650 Loss 3.9429 Accuracy 0.1441\n","Epoch 1 Batch 1700 Loss 3.9101 Accuracy 0.1464\n","Epoch 1 Batch 1750 Loss 3.8787 Accuracy 0.1486\n","Epoch 1 Batch 1800 Loss 3.8489 Accuracy 0.1506\n","Epoch 1 Batch 1850 Loss 3.8194 Accuracy 0.1527\n","Epoch 1 Batch 1900 Loss 3.7900 Accuracy 0.1547\n","Epoch 1 Batch 1950 Loss 3.7611 Accuracy 0.1567\n","Epoch 1 Batch 2000 Loss 3.7344 Accuracy 0.1587\n","Epoch 1 Batch 2050 Loss 3.7065 Accuracy 0.1607\n","Epoch 1 Batch 2100 Loss 3.6788 Accuracy 0.1626\n","Epoch 1 Batch 2150 Loss 3.6529 Accuracy 0.1645\n","Epoch 1 Batch 2200 Loss 3.6275 Accuracy 0.1664\n","Epoch 1 Batch 2250 Loss 3.6022 Accuracy 0.1683\n","Epoch 1 Batch 2300 Loss 3.5792 Accuracy 0.1703\n","Epoch 1 Batch 2350 Loss 3.5552 Accuracy 0.1722\n","Epoch 1 Batch 2400 Loss 3.5319 Accuracy 0.1740\n","Epoch 1 Batch 2450 Loss 3.5077 Accuracy 0.1759\n","Epoch 1 Batch 2500 Loss 3.4839 Accuracy 0.1778\n","Epoch 1 Batch 2550 Loss 3.4602 Accuracy 0.1798\n","Epoch 1 Batch 2600 Loss 3.4383 Accuracy 0.1817\n","Epoch 1 Batch 2650 Loss 3.4164 Accuracy 0.1836\n","Epoch 1 Batch 2700 Loss 3.3949 Accuracy 0.1855\n","Epoch 1 Batch 2750 Loss 3.3737 Accuracy 0.1874\n","Epoch 1 Batch 2800 Loss 3.3533 Accuracy 0.1893\n","Epoch 1 Batch 2850 Loss 3.3336 Accuracy 0.1912\n","Epoch 1 Batch 2900 Loss 3.3140 Accuracy 0.1931\n","Epoch 1 Batch 2950 Loss 3.2948 Accuracy 0.1950\n","Epoch 1 Batch 3000 Loss 3.2755 Accuracy 0.1969\n","Epoch 1 Batch 3050 Loss 3.2575 Accuracy 0.1989\n","Epoch 1 Batch 3100 Loss 3.2390 Accuracy 0.2007\n","Epoch 1 Batch 3150 Loss 3.2206 Accuracy 0.2026\n","Epoch 1 Batch 3200 Loss 3.2021 Accuracy 0.2045\n","Epoch 1 Batch 3250 Loss 3.1850 Accuracy 0.2062\n","Epoch 1 Batch 3300 Loss 3.1680 Accuracy 0.2081\n","Epoch 1 Batch 3350 Loss 3.1512 Accuracy 0.2100\n","Epoch 1 Batch 3400 Loss 3.1344 Accuracy 0.2118\n","Epoch 1 Batch 3450 Loss 3.1179 Accuracy 0.2136\n","Epoch 1 Batch 3500 Loss 3.1018 Accuracy 0.2154\n","Epoch 1 Batch 3550 Loss 3.0854 Accuracy 0.2172\n","Epoch 1 Batch 3600 Loss 3.0705 Accuracy 0.2189\n","Epoch 1 Batch 3650 Loss 3.0550 Accuracy 0.2206\n","Epoch 1 Batch 3700 Loss 3.0395 Accuracy 0.2223\n","Epoch 1 Batch 3750 Loss 3.0245 Accuracy 0.2240\n","Epoch 1 Batch 3800 Loss 3.0092 Accuracy 0.2257\n","Epoch 1 Batch 3850 Loss 2.9943 Accuracy 0.2274\n","Epoch 1 Batch 3900 Loss 2.9799 Accuracy 0.2291\n","Epoch 1 Batch 3950 Loss 2.9654 Accuracy 0.2307\n","Epoch 1 Batch 4000 Loss 2.9513 Accuracy 0.2323\n","Epoch 1 Batch 4050 Loss 2.9377 Accuracy 0.2339\n","Epoch 1 Batch 4100 Loss 2.9237 Accuracy 0.2355\n","Epoch 1 Batch 4150 Loss 2.9097 Accuracy 0.2371\n","Epoch 1 Batch 4200 Loss 2.8961 Accuracy 0.2388\n","Epoch 1 Batch 4250 Loss 2.8823 Accuracy 0.2405\n","Epoch 1 Batch 4300 Loss 2.8687 Accuracy 0.2422\n","Epoch 1 Batch 4350 Loss 2.8555 Accuracy 0.2438\n","Epoch 1 Batch 4400 Loss 2.8422 Accuracy 0.2454\n","Epoch 1 Batch 4450 Loss 2.8295 Accuracy 0.2471\n","Epoch 1 Batch 4500 Loss 2.8165 Accuracy 0.2487\n","Epoch 1 Batch 4550 Loss 2.8038 Accuracy 0.2503\n","Epoch 1 Batch 4600 Loss 2.7912 Accuracy 0.2519\n","Epoch 1 Batch 4650 Loss 2.7785 Accuracy 0.2535\n","Epoch 1 Batch 4700 Loss 2.7669 Accuracy 0.2549\n","Epoch 1 Batch 4750 Loss 2.7558 Accuracy 0.2562\n","Epoch 1 Batch 4800 Loss 2.7454 Accuracy 0.2574\n","Epoch 1 Batch 4850 Loss 2.7360 Accuracy 0.2586\n","Epoch 1 Batch 4900 Loss 2.7270 Accuracy 0.2596\n","Epoch 1 Batch 4950 Loss 2.7175 Accuracy 0.2606\n","Epoch 1 Batch 5000 Loss 2.7091 Accuracy 0.2616\n","Epoch 1 Batch 5050 Loss 2.7006 Accuracy 0.2625\n","Epoch 1 Batch 5100 Loss 2.6924 Accuracy 0.2634\n","Epoch 1 Batch 5150 Loss 2.6844 Accuracy 0.2643\n","Epoch 1 Batch 5200 Loss 2.6767 Accuracy 0.2651\n","Epoch 1 Batch 5250 Loss 2.6689 Accuracy 0.2660\n","Epoch 1 Batch 5300 Loss 2.6610 Accuracy 0.2669\n","Epoch 1 Batch 5350 Loss 2.6535 Accuracy 0.2676\n","Epoch 1 Batch 5400 Loss 2.6461 Accuracy 0.2685\n","Epoch 1 Batch 5450 Loss 2.6385 Accuracy 0.2693\n","Epoch 1 Batch 5500 Loss 2.6310 Accuracy 0.2702\n","Epoch 1 Batch 5550 Loss 2.6238 Accuracy 0.2709\n","Epoch 1 Batch 5600 Loss 2.6168 Accuracy 0.2717\n","Epoch 1 Batch 5650 Loss 2.6096 Accuracy 0.2725\n","Epoch 1 Batch 5700 Loss 2.6025 Accuracy 0.2733\n","Epoch 1 Batch 5750 Loss 2.5957 Accuracy 0.2740\n","Epoch 1 Batch 5800 Loss 2.5888 Accuracy 0.2747\n","Epoch 1 Batch 5850 Loss 2.5818 Accuracy 0.2755\n","Epoch 1 Batch 5900 Loss 2.5753 Accuracy 0.2762\n","Epoch 1 Batch 5950 Loss 2.5684 Accuracy 0.2769\n","Epoch 1 Batch 6000 Loss 2.5617 Accuracy 0.2775\n","Epoch 1 Batch 6050 Loss 2.5550 Accuracy 0.2782\n","Epoch 1 Batch 6100 Loss 2.5484 Accuracy 0.2788\n","Epoch 1 Batch 6150 Loss 2.5418 Accuracy 0.2795\n","Epoch 1 Batch 6200 Loss 2.5352 Accuracy 0.2801\n","Epoch 1 Batch 6250 Loss 2.5285 Accuracy 0.2808\n","Epoch 1 Batch 6300 Loss 2.5220 Accuracy 0.2815\n","Epoch 1 Batch 6350 Loss 2.5153 Accuracy 0.2821\n","Epoch 1 Batch 6400 Loss 2.5088 Accuracy 0.2828\n","Saving checkpoint for epoch 1 at ./drive/My Drive/projects/transformer/ckpt/ckpt-1\n","Time taken for 1 epoch: 6929.524331331253 secs\n","\n","Start of epoch 2\n","Epoch 2 Batch 0 Loss 2.0258 Accuracy 0.3750\n","Epoch 2 Batch 50 Loss 1.7327 Accuracy 0.3644\n","Epoch 2 Batch 100 Loss 1.7109 Accuracy 0.3688\n","Epoch 2 Batch 150 Loss 1.7022 Accuracy 0.3700\n","Epoch 2 Batch 200 Loss 1.6935 Accuracy 0.3724\n","Epoch 2 Batch 250 Loss 1.6935 Accuracy 0.3729\n","Epoch 2 Batch 300 Loss 1.6855 Accuracy 0.3741\n","Epoch 2 Batch 350 Loss 1.6840 Accuracy 0.3742\n","Epoch 2 Batch 400 Loss 1.6792 Accuracy 0.3745\n","Epoch 2 Batch 450 Loss 1.6728 Accuracy 0.3744\n","Epoch 2 Batch 500 Loss 1.6640 Accuracy 0.3748\n","Epoch 2 Batch 550 Loss 1.6610 Accuracy 0.3750\n","Epoch 2 Batch 600 Loss 1.6570 Accuracy 0.3748\n","Epoch 2 Batch 650 Loss 1.6525 Accuracy 0.3749\n","Epoch 2 Batch 700 Loss 1.6489 Accuracy 0.3752\n","Epoch 2 Batch 750 Loss 1.6430 Accuracy 0.3761\n","Epoch 2 Batch 800 Loss 1.6381 Accuracy 0.3770\n","Epoch 2 Batch 850 Loss 1.6334 Accuracy 0.3784\n","Epoch 2 Batch 900 Loss 1.6279 Accuracy 0.3795\n","Epoch 2 Batch 950 Loss 1.6213 Accuracy 0.3809\n","Epoch 2 Batch 1000 Loss 1.6138 Accuracy 0.3821\n","Epoch 2 Batch 1050 Loss 1.6065 Accuracy 0.3830\n","Epoch 2 Batch 1100 Loss 1.5986 Accuracy 0.3840\n","Epoch 2 Batch 1150 Loss 1.5903 Accuracy 0.3850\n","Epoch 2 Batch 1200 Loss 1.5839 Accuracy 0.3859\n","Epoch 2 Batch 1250 Loss 1.5765 Accuracy 0.3868\n","Epoch 2 Batch 1300 Loss 1.5689 Accuracy 0.3877\n","Epoch 2 Batch 1350 Loss 1.5619 Accuracy 0.3888\n","Epoch 2 Batch 1400 Loss 1.5552 Accuracy 0.3897\n","Epoch 2 Batch 1450 Loss 1.5487 Accuracy 0.3906\n","Epoch 2 Batch 1500 Loss 1.5418 Accuracy 0.3914\n","Epoch 2 Batch 1550 Loss 1.5345 Accuracy 0.3922\n","Epoch 2 Batch 1600 Loss 1.5278 Accuracy 0.3930\n","Epoch 2 Batch 1650 Loss 1.5219 Accuracy 0.3937\n","Epoch 2 Batch 1700 Loss 1.5164 Accuracy 0.3944\n","Epoch 2 Batch 1750 Loss 1.5103 Accuracy 0.3953\n","Epoch 2 Batch 1800 Loss 1.5046 Accuracy 0.3960\n","Epoch 2 Batch 1850 Loss 1.4990 Accuracy 0.3966\n","Epoch 2 Batch 1900 Loss 1.4934 Accuracy 0.3973\n","Epoch 2 Batch 1950 Loss 1.4878 Accuracy 0.3978\n","Epoch 2 Batch 2000 Loss 1.4824 Accuracy 0.3985\n","Epoch 2 Batch 2050 Loss 1.4774 Accuracy 0.3992\n","Epoch 2 Batch 2100 Loss 1.4720 Accuracy 0.3997\n","Epoch 2 Batch 2150 Loss 1.4665 Accuracy 0.4002\n","Epoch 2 Batch 2200 Loss 1.4607 Accuracy 0.4008\n","Epoch 2 Batch 2250 Loss 1.4566 Accuracy 0.4012\n","Epoch 2 Batch 2300 Loss 1.4520 Accuracy 0.4018\n","Epoch 2 Batch 2350 Loss 1.4470 Accuracy 0.4023\n","Epoch 2 Batch 2400 Loss 1.4426 Accuracy 0.4030\n","Epoch 2 Batch 2450 Loss 1.4379 Accuracy 0.4035\n","Epoch 2 Batch 2500 Loss 1.4326 Accuracy 0.4040\n","Epoch 2 Batch 2550 Loss 1.4276 Accuracy 0.4047\n","Epoch 2 Batch 2600 Loss 1.4222 Accuracy 0.4052\n","Epoch 2 Batch 2650 Loss 1.4175 Accuracy 0.4057\n","Epoch 2 Batch 2700 Loss 1.4134 Accuracy 0.4063\n","Epoch 2 Batch 2750 Loss 1.4094 Accuracy 0.4068\n","Epoch 2 Batch 2800 Loss 1.4060 Accuracy 0.4073\n","Epoch 2 Batch 2850 Loss 1.4029 Accuracy 0.4078\n","Epoch 2 Batch 2900 Loss 1.3995 Accuracy 0.4083\n","Epoch 2 Batch 2950 Loss 1.3963 Accuracy 0.4088\n","Epoch 2 Batch 3000 Loss 1.3931 Accuracy 0.4093\n","Epoch 2 Batch 3050 Loss 1.3903 Accuracy 0.4098\n","Epoch 2 Batch 3100 Loss 1.3871 Accuracy 0.4103\n","Epoch 2 Batch 3150 Loss 1.3838 Accuracy 0.4109\n","Epoch 2 Batch 3200 Loss 1.3808 Accuracy 0.4115\n","Epoch 2 Batch 3250 Loss 1.3787 Accuracy 0.4120\n","Epoch 2 Batch 3300 Loss 1.3756 Accuracy 0.4125\n","Epoch 2 Batch 3350 Loss 1.3728 Accuracy 0.4131\n","Epoch 2 Batch 3400 Loss 1.3701 Accuracy 0.4136\n","Epoch 2 Batch 3450 Loss 1.3673 Accuracy 0.4141\n","Epoch 2 Batch 3500 Loss 1.3647 Accuracy 0.4146\n","Epoch 2 Batch 3550 Loss 1.3619 Accuracy 0.4152\n","Epoch 2 Batch 3600 Loss 1.3593 Accuracy 0.4158\n","Epoch 2 Batch 3650 Loss 1.3564 Accuracy 0.4163\n","Epoch 2 Batch 3700 Loss 1.3539 Accuracy 0.4168\n","Epoch 2 Batch 3750 Loss 1.3516 Accuracy 0.4172\n","Epoch 2 Batch 3800 Loss 1.3489 Accuracy 0.4178\n","Epoch 2 Batch 3850 Loss 1.3462 Accuracy 0.4183\n","Epoch 2 Batch 3900 Loss 1.3435 Accuracy 0.4188\n","Epoch 2 Batch 3950 Loss 1.3410 Accuracy 0.4192\n","Epoch 2 Batch 4000 Loss 1.3385 Accuracy 0.4198\n","Epoch 2 Batch 4050 Loss 1.3359 Accuracy 0.4203\n","Epoch 2 Batch 4100 Loss 1.3336 Accuracy 0.4208\n","Epoch 2 Batch 4150 Loss 1.3308 Accuracy 0.4214\n","Epoch 2 Batch 4200 Loss 1.3281 Accuracy 0.4219\n","Epoch 2 Batch 4250 Loss 1.3254 Accuracy 0.4225\n","Epoch 2 Batch 4300 Loss 1.3228 Accuracy 0.4232\n","Epoch 2 Batch 4350 Loss 1.3198 Accuracy 0.4237\n","Epoch 2 Batch 4400 Loss 1.3170 Accuracy 0.4243\n","Epoch 2 Batch 4450 Loss 1.3145 Accuracy 0.4248\n","Epoch 2 Batch 4500 Loss 1.3118 Accuracy 0.4254\n","Epoch 2 Batch 4550 Loss 1.3094 Accuracy 0.4259\n","Epoch 2 Batch 4600 Loss 1.3067 Accuracy 0.4265\n","Epoch 2 Batch 4650 Loss 1.3043 Accuracy 0.4270\n","Epoch 2 Batch 4700 Loss 1.3030 Accuracy 0.4274\n","Epoch 2 Batch 4750 Loss 1.3023 Accuracy 0.4276\n","Epoch 2 Batch 4800 Loss 1.3020 Accuracy 0.4277\n","Epoch 2 Batch 4850 Loss 1.3020 Accuracy 0.4278\n","Epoch 2 Batch 4900 Loss 1.3025 Accuracy 0.4279\n","Epoch 2 Batch 4950 Loss 1.3033 Accuracy 0.4278\n","Epoch 2 Batch 5000 Loss 1.3041 Accuracy 0.4278\n","Epoch 2 Batch 5050 Loss 1.3050 Accuracy 0.4278\n","Epoch 2 Batch 5100 Loss 1.3059 Accuracy 0.4277\n","Epoch 2 Batch 5150 Loss 1.3072 Accuracy 0.4275\n","Epoch 2 Batch 5200 Loss 1.3081 Accuracy 0.4274\n","Epoch 2 Batch 5250 Loss 1.3095 Accuracy 0.4273\n","Epoch 2 Batch 5300 Loss 1.3105 Accuracy 0.4271\n","Epoch 2 Batch 5350 Loss 1.3116 Accuracy 0.4270\n","Epoch 2 Batch 5400 Loss 1.3129 Accuracy 0.4268\n","Epoch 2 Batch 5450 Loss 1.3140 Accuracy 0.4267\n","Epoch 2 Batch 5500 Loss 1.3150 Accuracy 0.4266\n","Epoch 2 Batch 5550 Loss 1.3160 Accuracy 0.4265\n","Epoch 2 Batch 5600 Loss 1.3169 Accuracy 0.4264\n","Epoch 2 Batch 5650 Loss 1.3179 Accuracy 0.4262\n","Epoch 2 Batch 5700 Loss 1.3190 Accuracy 0.4261\n","Epoch 2 Batch 5750 Loss 1.3200 Accuracy 0.4259\n","Epoch 2 Batch 5800 Loss 1.3210 Accuracy 0.4258\n","Epoch 2 Batch 5850 Loss 1.3220 Accuracy 0.4256\n","Epoch 2 Batch 5900 Loss 1.3231 Accuracy 0.4254\n","Epoch 2 Batch 5950 Loss 1.3239 Accuracy 0.4253\n","Epoch 2 Batch 6000 Loss 1.3248 Accuracy 0.4251\n","Epoch 2 Batch 6050 Loss 1.3255 Accuracy 0.4249\n","Epoch 2 Batch 6100 Loss 1.3263 Accuracy 0.4247\n","Epoch 2 Batch 6150 Loss 1.3271 Accuracy 0.4245\n","Epoch 2 Batch 6200 Loss 1.3276 Accuracy 0.4244\n","Epoch 2 Batch 6250 Loss 1.3280 Accuracy 0.4242\n","Epoch 2 Batch 6300 Loss 1.3286 Accuracy 0.4241\n","Epoch 2 Batch 6350 Loss 1.3292 Accuracy 0.4240\n","Epoch 2 Batch 6400 Loss 1.3298 Accuracy 0.4239\n","Saving checkpoint for epoch 2 at ./drive/My Drive/projects/transformer/ckpt/ckpt-2\n","Time taken for 1 epoch: 6909.9194276332855 secs\n","\n","Start of epoch 3\n","Epoch 3 Batch 0 Loss 1.5334 Accuracy 0.4112\n","Epoch 3 Batch 50 Loss 1.4306 Accuracy 0.4103\n","Epoch 3 Batch 100 Loss 1.4078 Accuracy 0.4127\n","Epoch 3 Batch 150 Loss 1.4130 Accuracy 0.4129\n","Epoch 3 Batch 200 Loss 1.4025 Accuracy 0.4139\n","Epoch 3 Batch 250 Loss 1.3950 Accuracy 0.4133\n","Epoch 3 Batch 300 Loss 1.3917 Accuracy 0.4135\n","Epoch 3 Batch 350 Loss 1.3876 Accuracy 0.4134\n","Epoch 3 Batch 400 Loss 1.3855 Accuracy 0.4135\n","Epoch 3 Batch 450 Loss 1.3816 Accuracy 0.4141\n","Epoch 3 Batch 500 Loss 1.3771 Accuracy 0.4144\n","Epoch 3 Batch 550 Loss 1.3729 Accuracy 0.4144\n","Epoch 3 Batch 600 Loss 1.3726 Accuracy 0.4143\n","Epoch 3 Batch 650 Loss 1.3723 Accuracy 0.4142\n","Epoch 3 Batch 700 Loss 1.3700 Accuracy 0.4143\n","Epoch 3 Batch 750 Loss 1.3671 Accuracy 0.4150\n","Epoch 3 Batch 800 Loss 1.3643 Accuracy 0.4159\n","Epoch 3 Batch 850 Loss 1.3588 Accuracy 0.4172\n","Epoch 3 Batch 900 Loss 1.3507 Accuracy 0.4184\n","Epoch 3 Batch 950 Loss 1.3446 Accuracy 0.4198\n","Epoch 3 Batch 1000 Loss 1.3374 Accuracy 0.4209\n","Epoch 3 Batch 1050 Loss 1.3307 Accuracy 0.4217\n","Epoch 3 Batch 1150 Loss 1.3179 Accuracy 0.4233\n","Epoch 3 Batch 1200 Loss 1.3131 Accuracy 0.4242\n","Epoch 3 Batch 1250 Loss 1.3067 Accuracy 0.4250\n","Epoch 3 Batch 1300 Loss 1.2997 Accuracy 0.4259\n","Epoch 3 Batch 1350 Loss 1.2936 Accuracy 0.4268\n","Epoch 3 Batch 1400 Loss 1.2876 Accuracy 0.4276\n","Epoch 3 Batch 1450 Loss 1.2822 Accuracy 0.4285\n","Epoch 3 Batch 1500 Loss 1.2764 Accuracy 0.4293\n","Epoch 3 Batch 1550 Loss 1.2709 Accuracy 0.4300\n","Epoch 3 Batch 1600 Loss 1.2661 Accuracy 0.4308\n","Epoch 3 Batch 1650 Loss 1.2612 Accuracy 0.4315\n","Epoch 3 Batch 1700 Loss 1.2560 Accuracy 0.4322\n","Epoch 3 Batch 1750 Loss 1.2513 Accuracy 0.4329\n","Epoch 3 Batch 1800 Loss 1.2466 Accuracy 0.4332\n","Epoch 3 Batch 1850 Loss 1.2416 Accuracy 0.4338\n","Epoch 3 Batch 1900 Loss 1.2374 Accuracy 0.4342\n","Epoch 3 Batch 1950 Loss 1.2328 Accuracy 0.4346\n","Epoch 3 Batch 2000 Loss 1.2280 Accuracy 0.4349\n","Epoch 3 Batch 2050 Loss 1.2240 Accuracy 0.4355\n","Epoch 3 Batch 2100 Loss 1.2198 Accuracy 0.4359\n","Epoch 3 Batch 2150 Loss 1.2161 Accuracy 0.4363\n","Epoch 3 Batch 2200 Loss 1.2115 Accuracy 0.4368\n","Epoch 3 Batch 2250 Loss 1.2075 Accuracy 0.4373\n","Epoch 3 Batch 2300 Loss 1.2035 Accuracy 0.4377\n","Epoch 3 Batch 2350 Loss 1.1994 Accuracy 0.4381\n","Epoch 3 Batch 2400 Loss 1.1958 Accuracy 0.4386\n","Epoch 3 Batch 2450 Loss 1.1920 Accuracy 0.4391\n","Epoch 3 Batch 2500 Loss 1.1879 Accuracy 0.4396\n","Epoch 3 Batch 2550 Loss 1.1840 Accuracy 0.4399\n","Epoch 3 Batch 2600 Loss 1.1800 Accuracy 0.4403\n","Epoch 3 Batch 2650 Loss 1.1767 Accuracy 0.4407\n","Epoch 3 Batch 2700 Loss 1.1740 Accuracy 0.4412\n","Epoch 3 Batch 2750 Loss 1.1717 Accuracy 0.4415\n","Epoch 3 Batch 2800 Loss 1.1691 Accuracy 0.4419\n","Epoch 3 Batch 2850 Loss 1.1663 Accuracy 0.4424\n","Epoch 3 Batch 2900 Loss 1.1638 Accuracy 0.4427\n","Epoch 3 Batch 2950 Loss 1.1617 Accuracy 0.4430\n","Epoch 3 Batch 3000 Loss 1.1597 Accuracy 0.4435\n","Epoch 3 Batch 3050 Loss 1.1576 Accuracy 0.4439\n","Epoch 3 Batch 3100 Loss 1.1553 Accuracy 0.4443\n","Epoch 3 Batch 3150 Loss 1.1530 Accuracy 0.4447\n","Epoch 3 Batch 3200 Loss 1.1511 Accuracy 0.4450\n","Epoch 3 Batch 3250 Loss 1.1493 Accuracy 0.4454\n","Epoch 3 Batch 3300 Loss 1.1477 Accuracy 0.4458\n","Epoch 3 Batch 3350 Loss 1.1459 Accuracy 0.4463\n","Epoch 3 Batch 3400 Loss 1.1443 Accuracy 0.4466\n","Epoch 3 Batch 3450 Loss 1.1427 Accuracy 0.4470\n","Epoch 3 Batch 3500 Loss 1.1409 Accuracy 0.4474\n","Epoch 3 Batch 3550 Loss 1.1393 Accuracy 0.4479\n","Epoch 3 Batch 3600 Loss 1.1373 Accuracy 0.4483\n","Epoch 3 Batch 3650 Loss 1.1352 Accuracy 0.4488\n","Epoch 3 Batch 3700 Loss 1.1335 Accuracy 0.4492\n","Epoch 3 Batch 3750 Loss 1.1318 Accuracy 0.4495\n","Epoch 3 Batch 3800 Loss 1.1299 Accuracy 0.4499\n","Epoch 3 Batch 3850 Loss 1.1280 Accuracy 0.4503\n","Epoch 3 Batch 3900 Loss 1.1260 Accuracy 0.4507\n","Epoch 3 Batch 3950 Loss 1.1241 Accuracy 0.4510\n","Epoch 3 Batch 4000 Loss 1.1223 Accuracy 0.4514\n","Epoch 3 Batch 4050 Loss 1.1207 Accuracy 0.4518\n","Epoch 3 Batch 4100 Loss 1.1189 Accuracy 0.4523\n","Epoch 3 Batch 4150 Loss 1.1168 Accuracy 0.4527\n","Epoch 3 Batch 4200 Loss 1.1149 Accuracy 0.4532\n","Epoch 3 Batch 4250 Loss 1.1131 Accuracy 0.4537\n","Epoch 3 Batch 4300 Loss 1.1113 Accuracy 0.4542\n","Epoch 3 Batch 4350 Loss 1.1095 Accuracy 0.4546\n","Epoch 3 Batch 4400 Loss 1.1077 Accuracy 0.4550\n","Epoch 3 Batch 4450 Loss 1.1061 Accuracy 0.4554\n","Epoch 3 Batch 4500 Loss 1.1046 Accuracy 0.4559\n","Epoch 3 Batch 4550 Loss 1.1031 Accuracy 0.4564\n","Epoch 3 Batch 4600 Loss 1.1012 Accuracy 0.4568\n","Epoch 3 Batch 4650 Loss 1.0998 Accuracy 0.4573\n","Epoch 3 Batch 4700 Loss 1.0992 Accuracy 0.4575\n","Epoch 3 Batch 4750 Loss 1.0987 Accuracy 0.4576\n","Epoch 3 Batch 4800 Loss 1.0991 Accuracy 0.4577\n","Epoch 3 Batch 4850 Loss 1.0999 Accuracy 0.4577\n","Epoch 3 Batch 4900 Loss 1.1008 Accuracy 0.4576\n","Epoch 3 Batch 4950 Loss 1.1023 Accuracy 0.4574\n","Epoch 3 Batch 5000 Loss 1.1038 Accuracy 0.4572\n","Epoch 3 Batch 5050 Loss 1.1057 Accuracy 0.4571\n","Epoch 3 Batch 5100 Loss 1.1075 Accuracy 0.4568\n","Epoch 3 Batch 5150 Loss 1.1095 Accuracy 0.4566\n","Epoch 3 Batch 5200 Loss 1.1115 Accuracy 0.4564\n","Epoch 3 Batch 5250 Loss 1.1135 Accuracy 0.4561\n","Epoch 3 Batch 5300 Loss 1.1151 Accuracy 0.4558\n","Epoch 3 Batch 5350 Loss 1.1171 Accuracy 0.4556\n","Epoch 3 Batch 5400 Loss 1.1188 Accuracy 0.4554\n","Epoch 3 Batch 5450 Loss 1.1208 Accuracy 0.4551\n","Epoch 3 Batch 5500 Loss 1.1224 Accuracy 0.4549\n","Epoch 3 Batch 5550 Loss 1.1242 Accuracy 0.4547\n","Epoch 3 Batch 5600 Loss 1.1263 Accuracy 0.4545\n","Epoch 3 Batch 5650 Loss 1.1279 Accuracy 0.4542\n","Epoch 3 Batch 5700 Loss 1.1296 Accuracy 0.4540\n","Epoch 3 Batch 5750 Loss 1.1312 Accuracy 0.4538\n","Epoch 3 Batch 5800 Loss 1.1330 Accuracy 0.4535\n","Epoch 3 Batch 5850 Loss 1.1347 Accuracy 0.4533\n","Epoch 3 Batch 5900 Loss 1.1363 Accuracy 0.4530\n","Epoch 3 Batch 5950 Loss 1.1378 Accuracy 0.4527\n","Epoch 3 Batch 6000 Loss 1.1390 Accuracy 0.4524\n","Epoch 3 Batch 6050 Loss 1.1404 Accuracy 0.4522\n","Epoch 3 Batch 6100 Loss 1.1417 Accuracy 0.4519\n","Epoch 3 Batch 6150 Loss 1.1432 Accuracy 0.4517\n","Epoch 3 Batch 6200 Loss 1.1447 Accuracy 0.4515\n","Epoch 3 Batch 6250 Loss 1.1460 Accuracy 0.4512\n","Epoch 3 Batch 6300 Loss 1.1469 Accuracy 0.4510\n","Epoch 3 Batch 6350 Loss 1.1479 Accuracy 0.4507\n","Epoch 3 Batch 6400 Loss 1.1490 Accuracy 0.4505\n","Saving checkpoint for epoch 3 at ./drive/My Drive/projects/transformer/ckpt/ckpt-3\n","Time taken for 1 epoch: 7108.120851516724 secs\n","\n","Start of epoch 4\n","Epoch 4 Batch 0 Loss 1.2837 Accuracy 0.4556\n","Epoch 4 Batch 50 Loss 1.2858 Accuracy 0.4318\n","Epoch 4 Batch 100 Loss 1.2836 Accuracy 0.4327\n","Epoch 4 Batch 150 Loss 1.2831 Accuracy 0.4315\n","Epoch 4 Batch 200 Loss 1.2805 Accuracy 0.4307\n","Epoch 4 Batch 250 Loss 1.2864 Accuracy 0.4296\n","Epoch 4 Batch 300 Loss 1.2798 Accuracy 0.4305\n","Epoch 4 Batch 350 Loss 1.2793 Accuracy 0.4307\n","Epoch 4 Batch 400 Loss 1.2781 Accuracy 0.4305\n","Epoch 4 Batch 450 Loss 1.2762 Accuracy 0.4305\n","Epoch 4 Batch 500 Loss 1.2733 Accuracy 0.4299\n","Epoch 4 Batch 550 Loss 1.2714 Accuracy 0.4301\n","Epoch 4 Batch 600 Loss 1.2713 Accuracy 0.4301\n","Epoch 4 Batch 650 Loss 1.2702 Accuracy 0.4302\n","Epoch 4 Batch 700 Loss 1.2683 Accuracy 0.4302\n","Epoch 4 Batch 750 Loss 1.2649 Accuracy 0.4305\n","Epoch 4 Batch 800 Loss 1.2612 Accuracy 0.4312\n","Epoch 4 Batch 850 Loss 1.2552 Accuracy 0.4323\n","Epoch 4 Batch 900 Loss 1.2494 Accuracy 0.4331\n","Epoch 4 Batch 950 Loss 1.2434 Accuracy 0.4344\n","Epoch 4 Batch 1000 Loss 1.2362 Accuracy 0.4356\n","Epoch 4 Batch 1050 Loss 1.2291 Accuracy 0.4365\n","Epoch 4 Batch 1100 Loss 1.2219 Accuracy 0.4372\n","Epoch 4 Batch 1150 Loss 1.2163 Accuracy 0.4378\n","Epoch 4 Batch 1200 Loss 1.2106 Accuracy 0.4387\n","Epoch 4 Batch 1250 Loss 1.2045 Accuracy 0.4397\n","Epoch 4 Batch 1300 Loss 1.1990 Accuracy 0.4405\n","Epoch 4 Batch 1350 Loss 1.1933 Accuracy 0.4416\n","Epoch 4 Batch 1400 Loss 1.1868 Accuracy 0.4423\n","Epoch 4 Batch 1450 Loss 1.1808 Accuracy 0.4432\n","Epoch 4 Batch 1500 Loss 1.1757 Accuracy 0.4439\n","Epoch 4 Batch 1550 Loss 1.1709 Accuracy 0.4447\n","Epoch 4 Batch 1600 Loss 1.1659 Accuracy 0.4453\n","Epoch 4 Batch 1650 Loss 1.1616 Accuracy 0.4460\n","Epoch 4 Batch 1700 Loss 1.1568 Accuracy 0.4468\n","Epoch 4 Batch 1750 Loss 1.1528 Accuracy 0.4475\n","Epoch 4 Batch 1800 Loss 1.1481 Accuracy 0.4480\n","Epoch 4 Batch 1850 Loss 1.1431 Accuracy 0.4485\n","Epoch 4 Batch 1900 Loss 1.1381 Accuracy 0.4490\n","Epoch 4 Batch 1950 Loss 1.1342 Accuracy 0.4495\n","Epoch 4 Batch 2000 Loss 1.1303 Accuracy 0.4498\n","Epoch 4 Batch 2050 Loss 1.1264 Accuracy 0.4502\n","Epoch 4 Batch 2100 Loss 1.1227 Accuracy 0.4507\n","Epoch 4 Batch 2150 Loss 1.1189 Accuracy 0.4511\n","Epoch 4 Batch 2200 Loss 1.1151 Accuracy 0.4513\n","Epoch 4 Batch 2250 Loss 1.1118 Accuracy 0.4517\n","Epoch 4 Batch 2300 Loss 1.1087 Accuracy 0.4521\n","Epoch 4 Batch 2350 Loss 1.1054 Accuracy 0.4526\n","Epoch 4 Batch 2400 Loss 1.1017 Accuracy 0.4531\n","Epoch 4 Batch 2450 Loss 1.0981 Accuracy 0.4535\n","Epoch 4 Batch 2500 Loss 1.0941 Accuracy 0.4539\n","Epoch 4 Batch 2550 Loss 1.0901 Accuracy 0.4544\n","Epoch 4 Batch 2600 Loss 1.0869 Accuracy 0.4548\n","Epoch 4 Batch 2650 Loss 1.0841 Accuracy 0.4551\n","Epoch 4 Batch 2700 Loss 1.0809 Accuracy 0.4555\n","Epoch 4 Batch 2750 Loss 1.0783 Accuracy 0.4559\n","Epoch 4 Batch 2800 Loss 1.0756 Accuracy 0.4563\n","Epoch 4 Batch 2850 Loss 1.0731 Accuracy 0.4567\n","Epoch 4 Batch 2900 Loss 1.0713 Accuracy 0.4571\n","Epoch 4 Batch 2950 Loss 1.0689 Accuracy 0.4573\n","Epoch 4 Batch 3000 Loss 1.0670 Accuracy 0.4576\n","Epoch 4 Batch 3050 Loss 1.0651 Accuracy 0.4580\n","Epoch 4 Batch 3100 Loss 1.0634 Accuracy 0.4584\n","Epoch 4 Batch 3150 Loss 1.0614 Accuracy 0.4588\n","Epoch 4 Batch 3200 Loss 1.0597 Accuracy 0.4591\n","Epoch 4 Batch 3250 Loss 1.0576 Accuracy 0.4595\n","Epoch 4 Batch 3300 Loss 1.0558 Accuracy 0.4599\n","Epoch 4 Batch 3350 Loss 1.0544 Accuracy 0.4602\n","Epoch 4 Batch 3400 Loss 1.0530 Accuracy 0.4606\n","Epoch 4 Batch 3450 Loss 1.0517 Accuracy 0.4610\n","Epoch 4 Batch 3500 Loss 1.0501 Accuracy 0.4613\n","Epoch 4 Batch 3550 Loss 1.0484 Accuracy 0.4617\n","Epoch 4 Batch 3600 Loss 1.0470 Accuracy 0.4621\n","Epoch 4 Batch 3650 Loss 1.0455 Accuracy 0.4624\n","Epoch 4 Batch 3700 Loss 1.0438 Accuracy 0.4628\n","Epoch 4 Batch 3750 Loss 1.0421 Accuracy 0.4631\n","Epoch 4 Batch 3800 Loss 1.0406 Accuracy 0.4635\n","Epoch 4 Batch 3850 Loss 1.0389 Accuracy 0.4639\n","Epoch 4 Batch 3900 Loss 1.0378 Accuracy 0.4642\n","Epoch 4 Batch 3950 Loss 1.0362 Accuracy 0.4645\n","Epoch 4 Batch 4000 Loss 1.0346 Accuracy 0.4648\n","Epoch 4 Batch 4050 Loss 1.0331 Accuracy 0.4653\n","Epoch 4 Batch 4100 Loss 1.0315 Accuracy 0.4656\n","Epoch 4 Batch 4150 Loss 1.0298 Accuracy 0.4661\n","Epoch 4 Batch 4200 Loss 1.0279 Accuracy 0.4665\n","Epoch 4 Batch 4250 Loss 1.0264 Accuracy 0.4670\n","Epoch 4 Batch 4300 Loss 1.0248 Accuracy 0.4674\n","Epoch 4 Batch 4350 Loss 1.0234 Accuracy 0.4679\n","Epoch 4 Batch 4400 Loss 1.0219 Accuracy 0.4682\n","Epoch 4 Batch 4450 Loss 1.0203 Accuracy 0.4686\n","Epoch 4 Batch 4500 Loss 1.0192 Accuracy 0.4691\n","Epoch 4 Batch 4550 Loss 1.0177 Accuracy 0.4695\n","Epoch 4 Batch 4600 Loss 1.0162 Accuracy 0.4699\n","Epoch 4 Batch 4650 Loss 1.0151 Accuracy 0.4702\n","Epoch 4 Batch 4700 Loss 1.0145 Accuracy 0.4704\n","Epoch 4 Batch 4750 Loss 1.0144 Accuracy 0.4705\n","Epoch 4 Batch 4800 Loss 1.0150 Accuracy 0.4705\n","Epoch 4 Batch 4850 Loss 1.0161 Accuracy 0.4705\n","Epoch 4 Batch 4900 Loss 1.0173 Accuracy 0.4704\n","Epoch 4 Batch 4950 Loss 1.0191 Accuracy 0.4703\n","Epoch 4 Batch 5000 Loss 1.0206 Accuracy 0.4701\n","Epoch 4 Batch 5050 Loss 1.0225 Accuracy 0.4699\n","Epoch 4 Batch 5100 Loss 1.0241 Accuracy 0.4697\n","Epoch 4 Batch 5150 Loss 1.0264 Accuracy 0.4695\n","Epoch 4 Batch 5200 Loss 1.0286 Accuracy 0.4692\n","Epoch 4 Batch 5250 Loss 1.0309 Accuracy 0.4690\n","Epoch 4 Batch 5300 Loss 1.0329 Accuracy 0.4687\n","Epoch 4 Batch 5350 Loss 1.0349 Accuracy 0.4685\n","Epoch 4 Batch 5400 Loss 1.0371 Accuracy 0.4682\n","Epoch 4 Batch 5450 Loss 1.0389 Accuracy 0.4679\n","Epoch 4 Batch 5500 Loss 1.0407 Accuracy 0.4676\n","Epoch 4 Batch 5550 Loss 1.0423 Accuracy 0.4674\n","Epoch 4 Batch 5600 Loss 1.0441 Accuracy 0.4671\n","Epoch 4 Batch 5650 Loss 1.0459 Accuracy 0.4669\n","Epoch 4 Batch 5700 Loss 1.0477 Accuracy 0.4666\n","Epoch 4 Batch 5750 Loss 1.0499 Accuracy 0.4663\n","Epoch 4 Batch 5800 Loss 1.0515 Accuracy 0.4660\n","Epoch 4 Batch 5850 Loss 1.0534 Accuracy 0.4657\n","Epoch 4 Batch 5900 Loss 1.0554 Accuracy 0.4654\n","Epoch 4 Batch 5950 Loss 1.0571 Accuracy 0.4651\n","Epoch 4 Batch 6000 Loss 1.0589 Accuracy 0.4647\n","Epoch 4 Batch 6050 Loss 1.0605 Accuracy 0.4644\n","Epoch 4 Batch 6100 Loss 1.0620 Accuracy 0.4641\n","Epoch 4 Batch 6150 Loss 1.0633 Accuracy 0.4639\n","Epoch 4 Batch 6200 Loss 1.0649 Accuracy 0.4636\n","Epoch 4 Batch 6250 Loss 1.0663 Accuracy 0.4634\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nmzyRwDrRGdq"},"source":["# Evaluating"]},{"cell_type":"code","metadata":{"id":"cNHwJJrz3lPB"},"source":["def evaluate(inp_sentence):\n","    inp_sentence = \\\n","        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n","    enc_input = tf.expand_dims(inp_sentence, axis=0)\n","    \n","    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n","    \n","    for _ in range(MAX_LENGTH):\n","        predictions = transformer(enc_input, output, False)\n","        \n","        prediction = predictions[:, -1:, :]\n","        \n","        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n","        \n","        if predicted_id == VOCAB_SIZE_ES-1:\n","            return tf.squeeze(output, axis=0)\n","        \n","        output = tf.concat([output, predicted_id], axis=-1)\n","        \n","    return tf.squeeze(output, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6VeFKrE6Kdx"},"source":["def translate(sentence):\n","    output = evaluate(sentence).numpy()\n","    \n","    predicted_sentence = tokenizer_es.decode(\n","        [i for i in output if i < VOCAB_SIZE_ES-2]\n","    )\n","    \n","    print(\"Input: {}\".format(sentence))\n","    print(\"Predicted translation: {}\".format(predicted_sentence))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdoWKbCP7Czs"},"source":["translate(\"This is a really powerful tool!\")"],"execution_count":null,"outputs":[]}]}